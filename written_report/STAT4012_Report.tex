\documentclass[11pt,a4paper]{article}
\input{myPreliminary}

\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}

\usepackage[table]{xcolor}
%------------------------------------------------------------------------------

\begin{document}
    
    % Page 0 for names and table of contents
    \thispagestyle{empty}
    \pagenumbering{gobble} 
    \title{\textsc{STAT 4012} -- Statistical Principles of Deep Learning with Business \\ Group Project}
    \author{
        KWOK Ho Hin (SID: \texttt{1155126159}) \\
        LAI Tsz Chun (SID: \texttt{1155125208}) \\
        LAM Wai Chiu (SID: \texttt{1155152095}) \\
        LAW Yiu Leung Eric (SID: \texttt{1155149315}) \\
        TSOI Tung Sing (SID: \texttt{1155127274}) \\
        LO Chak Kin Steven (SID: \texttt{1155125491})
    }
    \date{\today}
    \maketitle
    
    \pagestyle{plain} 
    \pagenumbering{roman}
    
    \tableofcontents
    \listoffigures
    \listoftables
    
    \newpage
    
    % Section 1
    \pagenumbering{arabic}
    \pagestyle{fancy}
    \setcounter{page}{1}
    
    % Emotion Detection
    \section{Real Time Emotion Detection Deep Learning Model}
    \textbf{This section is contributed by LAI Tsz Chun (SID: 1155125208), LAM Wai Chiu (SID: 1155152095) and LAW Yiu Leung Eric (SID: 1155149315).}
    
    \subsection{Introduction}
    The study of Convolutional Neural Network (CNN) in this course has inspired us the importance of CNN in image classification. We further extend our inspiration to facial expression recognition. Emotion serves as an immediate means for humans to interact and sometimes it also affects human behavior. Hence, we aim to replicate human emotional recognition using CNN in a real time manner. \\
    \\
    We designed to build an artificial intelligence (AI) that consists of two models. The first model is a face detection model borrowed from MTCNN. The second model is our custom CNN model for facial expression recognition. The main objective is to recognize seven basic emotional expressions (Angry, Disgust, Fear, Happy, Sad and Surprise, Neutral) by our custom model with a satisfactory accuracy rate. A demonstration using our model in real time and a model evaluation are also covered to see the performance of the model.

    
    \subsection{Dataset}
    \href{https://www.kaggle.com/datasets/msambare/fer2013}{FER-2013} \cite{FER2013} dataset from Kaggle is selected for model training. The data consists of $48 \times 48$ pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image. The faces are categorized into seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Neutral, 5=Sad, 6=Surprise). The training set consists of 28,709 examples and the public test set consists of 3,589 examples. 
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.9\textwidth]{emotion\_detection/plot/train.pdf}
        \caption{Training Dataset}
        \label{fig:train_dataset}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.9\textwidth]{emotion_detection/plot/test.pdf}
        \caption{Testing Dataset}
        \label{fig:test_dataset}
    \end{figure}
    
    % \subsection{Project Description}
    
    \subsection{Data Preparation for Model Training}
    The data are stored in \texttt{.jpg} format, in order to use them, we must perform pre-processing to the dataset. We acknowledge that \texttt{Tensorflow} does provide a function \texttt{tf.keras.preprocessing.image.ImageDataGenerator}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator}} for loading image files from folders into data generator, which only load the image into tensor when in needed. However, we insist to save the data in \texttt{NumPy} format in order to speed up training process. Yet, this method have some drawbacks, which occupies about 2 GB of RAM for loading datasets before training, and extra storage space in hard disk.
    \begin{enumerate}
        \item Read the images using \texttt{OpenCV}.
        \item Normalized the grayscale from [0, 255] to [0, 1] for increasing training speed.
        \item Stack up 1-channel grayscale array into 3-channel RGB array, as the final use would be RGB pictures.
        \item Encode the labels.
        \item Save the array to \texttt{NumPy} array, then save as \texttt{Pickle} file.
    \end{enumerate}
    Noted that the dataset have already split into training and testing datasets from Kaggle, we do not need to split them. \\
    \\
    \textbf{Implementation:} \texttt{emotion\_detection\textbackslash src\textbackslash read\_data.ipynb}
        
    % \subsubsection{Create Numpy Array}
    
    % \subsubsection{Splitting Training Dataset and Testing Dataset}
    
    \subsection{Model Design}
    We intended to build a real-time emotion detection AI, so there are two separated models needed, namely face detection model and emotion detection model. Flow of the model:
    \begin{enumerate}
        \item Detect face(s) in a photo.
        \item Capture pixels of the face(s) and resize to $48 \times 48$.
        \item Classify emotion of the faces(s).
    \end{enumerate}
    
    % \newpage
    \subsubsection{MTCNN Face Detection Model}
    MTCNN is a python (pip) library written by Github user \href{https://github.com/ipazc/mtcnn}{ipacz} \cite{MTCNN}, which implements the paper by Zhang, Kaipeng et al. “Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.” \cite{1604.02878} In this paper, they propose a deep cascaded multi-task framework using different features of “sub-models” to each boost their correlating strengths. We choose MTCNN model for detecting faces in picture.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.8\textwidth]{written_report/pictures/mtcnn.jpg}
        \caption{Face Detection using MTCNN}
        \label{fig:MTCNN}
    \end{figure}
    
    \subsubsection{Augmentation}
    Too avoid over fitting, we use a technique called augmentation to increase the diversity of your training set by applying random flip and slight rotation.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.5\textwidth]{emotion_detection/plot/augmentation.pdf}
        \caption{Augmentation}
        \label{fig:augmentation}
    \end{figure}
    
    \subsubsection{Emotion Detection Model}
    We create and train a custom CNN model. After the augmentation function mentioned above, the pixels are passing through 5 blocks of convolution layers and 2 hidden fully-connected layers. All convolution layers and hidden layers  are using RELU activation, expect the output layer is using softmax activation. \\
    \\
    Block 1 and 2 each consists 2 layers of CNN layers, while block 3 to 5 each consists 3 layers of CNN layers, the filter size is increasing block by block from 64 to 512. In additions, each block has it own batch normalization layers, and from block 2 to 5, l2 regularization is used for kernel regularization. \\
    \\
    \textbf{Implementation:} \texttt{emotion\_detection\textbackslash src\textbackslash model\_train.ipynb}
    
    \subsubsection{Compile Model}
    The loss function is set to be sparse categorical cross-entropy, which is most suitable for multi-classes classification artificial neural network (ANN). Also, Adam is used as the optimizer for model training, with a common learning rate $\ng = 10^{-4}$. \\
    \\
    In total, this model consists 33,631,815 parameters, in which 33,628,871 of them are trainable and 2,944 are non-trainable (parameters for batch normalization).
    
    \subsubsection{Model Architecture}
    The architecture of our CNN model is as next page.
    \begin{figure}[H]
        \centering
        \begin{minipage}[b]{.4\textwidth}
            \centering
            \includegraphics[height = 0.9\textheight]{written_report/pictures/model_1.pdf}
        \end{minipage}
        \begin{minipage}[b]{.4\textwidth}
            \centering
            \includegraphics[height = 0.9\textheight]{written_report/pictures/model_2.pdf}
        \end{minipage}
        \caption{Model Architecture}
        \label{fig:model}
    \end{figure}
    
    \subsection{Model Training}
    \subsubsection{Training Specifications}
    We aim to train the model to an optimal point, neither overfiting nor underfitting. Thus, we set to train 200 epochs, but training procedure is expected to be stopped early, if the validation accuracy does not improve after 15 epochs. The final model would the one which has best validation accuracy. \\
    \\
    Training a very deep CNN requires huge amount of GPU power, in which non of our group member could afford it. Thanks to Google, they offer \href{https://research.google.com/colaboratory/}{Colab} which let us to use their GPU for free.
    
    \subsubsection{Model Evaluation}
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.9\textwidth]{emotion_detection/plot/history.pdf}
        \caption{Loss and Accuracy}
        \label{fig:loss_acc}
    \end{figure}
    Early stopping was trigger after 54 epochs, the model eventually achieved valid accuracy at around 0.58. \\
    With a \href{https://www.nvidia.com/en-us/data-center/tesla-t4/}{Tesla T4}\footnote{Nvidia Tesla T4 is optimized for mainstream computing environments and features multi-precision Turing Tensor Cores and new RT Cores} GPU, each epoch requires around 70 seconds, in total around an hour for complete training, which is reasonable.
    
    \newpage
    \subsection{Demonstration}
    
    % \subsubsection{Import Video}
    This AI emotion detection solution not only support real time detection, it also allows end-users input a video for detection. For demonstration, we use a video clip from \href{https://www.youtube.com/watch?v=myjEoDypUD8&pbjreload=102}{94th Academy Awards}\footnote{the video clip is adjusted to 3 fps}. With sufficient resolutions, the AI could easily detect happy faces.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.7\textwidth]{written_report/pictures/happy.png}
        \caption{Happy Faces}
        \label{fig:happy}
    \end{figure}
    \noindent
    \textbf{Implementation:} \texttt{emotion\_detection\textbackslash src\textbackslash video.py} \\
    \\
    % \subsubsection{Live Webcam}
    Back to our original objective of real time emotion detection, we use laptop webcam to capture real-time image, then pass it through into the model. The model runs pretty well in terms of performance, expect this demonstration is running on CPU, which results in very low frame rate.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.5\textwidth]{written_report/pictures/live_demo.png}
        \caption{Live Webcam Demonstration}
        \label{fig:live_webcam}
    \end{figure}
    \noindent
    \textbf{Implementation:} \texttt{emotion\_detection\textbackslash src\textbackslash real\_time\_webcam.py}
    
    \subsection{Discussion}
    \subsubsection{Basic Concept of Classification Evaluation}
    In this part, we will evaluate the performance of our CNN model by different indicators, namely confusion matrix, accuracy, precision, recall and F1-score. \\
    \\
    \textbf{Confusion matrix}: confusion matrix can show the summary of prediction and will be used to calculate accuracy, precision, recall and F1-score. 
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c}
             & \multicolumn{2}{c}{\textbf{True Class}} \\
            \hline
            % \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{Predicted Class}}}
            \multirow{2}{*}{\textbf{Predicted Class}} & True Positive (TP) & False Positive (FP) \\
            \cline{2-3}
             & False Negative (FN) & True Negative (TN)
        \end{tabular}
        \caption{Confusion Matrix}
        \label{tab:confusion_matrix}
    \end{table}
    \noindent
    Since our project is performing multi-class classification, an example of multi-class classification confusion matrix that an angry image is predicted as angry will be given as follows:
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c|c|c|c|c|c|c}
             & & \multicolumn{7}{c}{\textbf{True Class}} \\
            \cline{3-9}
             & & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise \\
            \cline{2-9}
            \parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\textbf{Predicted Class}}}} & Angry & TP & \multicolumn{6}{c}{FP} \\
            \cline{2-9}
             & Disgust & \multirow{6}{*}{FN} & \multicolumn{6}{c}{\multirow{6}{*}{TN}} \\ \cline{2-2}
             & Fear & \\ \cline{2-2}
             & Happy & \\ \cline{2-2}
             & Neutral & \\  \cline{2-2}
             & Sad & \\ \cline{2-2}
             & Surprise & 
        \end{tabular}
        \caption{Multi-Class Confusion Matrix}
        \label{tab:multi_class_confusion_matrix}
    \end{table}
    
    \subsubsection{Model Evaluation}
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
             & & \multicolumn{7}{c}{\textbf{True Class}} \\ \cline{3-9}
             & & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise \\ \cline{2-10}
            \parbox[t]{2mm}{\multirow{7}{*}{\rotatebox[origin=c]{90}{\textbf{Predicted Class}}}} & Angry & 557  &       9  &      59  &      53  &     134  &     127  &      19 & 958 \\ \cline{2-10}
             & Disgust & 35  &      51  &       8  &       4  &       3  &      10  &       0 & 111 \\ \cline{2-10}
             & Fear & 143  &      11  &     293  &      46  &     184  &     260  &      87 & 1024 \\ \cline{2-10}
             & Happy & 45  &       2  &      19  &    1542  &      94  &      47  &      25 & 1774 \\ \cline{2-10}
             & Neutral & 63  &       4  &      21  &      76  &     888  &     169  &      12 & 1233 \\ \cline{2-10}
             & Sad & 136  &      12  &      72  &      51  &     317  &     647  &      12 & 1247 \\ \cline{2-10}
             & Surprise & 32  &       1  &      74  &      58  &      70  &      14  &     582 & 831 \\ \cline{2-10}
              & & 1011 & 90 & 546 & 1830 & 1690 & 1274 & 737 & 7178
        \end{tabular}
        \caption{Prediction Outcome}
        \label{tab:prediction_outcome}
    \end{table}
    
    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c|c|c|c|c|c}
             & Angry & Disgust & Fear & Happy & Neutral & Sad & Surprise \\ \hline \hline
            Accuracy & 0.8809 &  0.9862 &  0.8629 &  0.9276 &  0.8402 &  0.8291 &  0.9437 \\ \hline
            Precision & 0.5814 &  0.4595 &  0.2861 &  0.8692 &  0.7202 &  0.5188 &  0.7004 \\ \hline
            Recall & 0.5509 &  0.5667 &  0.5366 &  0.8426 &  0.5254 &  0.5078 &  0.7897 \\ \hline
            F1-score & 0.5658 &  0.5075 &  0.3732 &  0.8557 &  0.6076 &  0.5133 &  0.7423 \\
        \end{tabular}
        \caption{Accuracy, Precision, Recall and F1-score}
        \label{tab:score}
    \end{table}
    \noindent
    In terms of accuracy, each class has great performance with all over 82\% accuracy. However, only using accuracy could not truly evaluate the overall performance, thus, we take F1-score to access performance among different classes. Form the table above, we can see that the performance of class \textbf{Fear} is the worst while the performance of class \textbf{Happy} is the best. The performance different of them is relatively large 0.3732 vs 0.8557. The reason may be \textbf{Fear} faces have similar characteristic with \textbf{Angry}, \textbf{Sad} and \textbf{Surprise}, we can see in Table \ref{tab:prediction_outcome} that the mentioned three classes also have many samples mislabeled as \textbf{Fear}. In such case, the CNN model is not able to identify unique characteristics to differentiate those classes.
    
    \subsection{Conclusion}
    In this part of our project, we have developed a CNN model to perform facial recognition instantly. There are seven emotions can be classified, namely, angry, disgust, fear, happy, sad, surprise and neutral. The FER-2013 dataset is used which contain 28,709 examples in training set and 3,589 examples in testing set. After we train the model, around 60\% testing images can be correctly classified. There are some factors will affect the classification accuracy rate of the model, like the quality of the images, the resolution of the images in each emotion class, mislabeling in the dataset, etc. To improve the accuracy, we can relabel the dataset to avoid misleading result, which, however, is extremely time consuming. The other way to make some possible improvements is to try other models, like recurrent neural network (RNN) or use some advance techniques, like Cycle Generative Adversarial Network (Cycle GAN), VGG-Face model and residual neural network (resnet). \\
    \\
    Facial recognition is used in different area nowadays which bring a huge benefit to our society. One big usage is customer service. Facial recognition can allow to record the emotion of the customer. The company can provide more training to the staffs if a certain proportion of customers get bad emotions after the service. Facial recognition can also assist to interview candidate. The trained model can determine the emotion of candidate automatically and report to the human recruiters which can reduce the cost. Our group believe that facial recognition will be more commonly used in the future.

    
    \newpage
    \section{Cryptocurrencies Pairs Trading}
    \textbf{This section is contributed by KWOK Ho Hin (SID: 1155126159), TSOI Tung Sing (SID: 1155127274) and LO Chak Kin Steven (SID: 1155125491).}
    
    \subsection{Introduction}
    With the rapid advancement in Deep Learning, experts have been searching for applications of deep learning models in various industries, especially in Finance. Deep Learning models, particularly LSTM model, have been used extensively in developing quantitative trading strategies. Typically, researchers tries to use LSTM to predict stock prices, or implied volatility in the case of options. \\
    \\
    In this project, we aimed to incorporate LSTM model in a Crypto Pairs Trading Strategy \cite{pairs_trading}. We first identified potential cointegrated pairs (token0, token1) and their respective hedge ratio ($\beta$). Then we tried to use LSTM to predict the spread of the pair (i.e. token0 – $\beta~\cdot$ token1) using only price data. A simple pair trading strategy can be designed by keeping track of the long-term mean ($\mu$) and setting a threshold ($e$) – we may buy the spread when it is below $\mu - e$ and sell it when it is above $\mu + e$. It is under the assumption that the cointegrated relationship between the two token will remain unchanged, hence the spread will be a stationary time series and will revert to its long-term mean mu.
    
    \subsection{Dataset}
    We have collected 10 months of minute-frequency data for 16 tokens with high market capitalization and trading volumes from \href{https://www.binance.com/en}{Binance} api. It includes candlesticks data, stored in OHLCV\footnote{OHLCV is an aggregated form of market data standing for Open, High, Low, Close and Volume.} format, aggregated in minute timeframe. 
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
            BTC & ETH & DOGE & BNB \\
            \hline
            XRP & TRX & ZIL & ADA \\
            \hline
            WAVES & ETC & LTC & MATIC \\
            \hline
            LINK & EOS & ATOM & XLM &
            \hline
        \end{tabular}
        \caption{Full List of Tokens}
        \label{tab:crypto_tokens}
    \end{table}
    
    \subsection{Data Pre-processing / Feature Engineering}
    Before getting into the model, we processed the data in the following ways:
    \begin{enumerate}
        \item Front-filling missing data (if there’s any)
        \item Compute log returns
        \item Generate addition features (RSI, EMA, rolling-STD of different window length) 
        \item Split the dataset into pre-data (for cointegration test, 1 month) and data (for training and testing, remaining 9 months)
    \end{enumerate}
    
    \subsection{Model Design}
    We adopt a three-step process for our complete model. The flow of our model is as follows:
    \begin{enumerate}
        \item 
            Use nearest-neighbour algorithm with a cross-sectional snapshot (the first observation in pre-data for all 16 tokens) to find the two nearest neighbours for each of the tokens.
        \item 
            Pair tokens up with their corresponding neighbours and conduct augmented Engle-Granger Two-Step Cointegration Test on each pair. Rank them in ascending order by the t-statistics.
        \item 
            Compute the spread and use a LSTM model in predicting the upcoming movement of the spread
    \end{enumerate}
    
    \subsubsection{Nearest Neighbour}
    To ease our computation, instead of looping through all the combination of pairs for the 16 tokens (total of $16 \times 15 ~/~ 2 = 120$ pairs) and conduct cointegration test one by one, we form potential pairs by matching them with their two nearest neighbours. By feeding in a cross-sectional snapshot of our data into the nearest-neighbour Algorithm, we identify 38 potential cointegrated pairs, which is less than a third of the original pairs.
    \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
            \hline
            DOGE - ZIL & LINK - BNB & ADA - XRP \\
            \hline
            MATIC - BNB & ATOM - ZIL & LTC - DOGE &
            \hline
        \end{tabular}
        \caption{Example of the Pairs}
        \label{tab:crypto_pairs}
    \end{table}
    
    % ** In addition to the table, I will try to make a graph of the pairs with T-SNE **
    % But need some time to see see how it works
    
    \subsubsection{Cointegration Test}
    For each of the 38 potential pairs, we conduct an augmented Engle-Granger two step cointegration test which, by the name, consists of two steps.
    \begin{enumerate}
        \item Estimating the long-run equilibrium of the two series with OLS regression 
        \item Conduct a unit root test (Augmented Dicky Fuller Test) on the OLS residuals
    \end{enumerate}
    \\
    The null hypothesis of the test is that there is no cointegrating relationship while the alternative hypothesis suggests that there is cointegrating relationship between the two. We reject the null hypothesis at a 5\% significance level when the $p$-value is smaller than 0.05. \\
    \\
    There are a total of 13 pairs that we successfully reject the null hypothesis at 5\% significance level.
    \begin{table}[H]
        \centering
        \begin{tabular}{l|r}
            pairs & tstat \\
            \hline \hline
            [EOS, ZIL] & -5.388334 &
            [EOS, DOGE] & -5.311655 &
            [XRP, BNB] & -4.886678 &
            [XLM, ZIL] & -4.370721 &
            [LINK, BNB] & -4.352107 &
            [LTC, ZIL] & -4.210387 &
            [ADA, LINK] & -4.151885 &
            [DOGE, ZIL] & -4.104011 &
            [LTC, DOGE] & -3.956726 &
            [ADA, XRP] & -3.704049
        \end{tabular}
        \caption{Sorted in Ascending Order by t-statistics}
        \label{tab:crypto_tstat}
    \end{table}
    
    \subsubsection{LSTM}
    LSTM is one of the neural networks that is commonly used in financial analysis. LSTM consists of four different parts of the components, namely forget gate, input gate, cell state and the output cell. The forget gate is the part which is responsible for deciding whether the information from the previous step is kept or not by using the sigmoid layer. Also, the cell state enables LSTM model to deal with a long sequence problem. Those two parts make LSTM different from other neural networks.
    
    \subsection{LSTM Model Description}
    \subsubsection{Model Setting}
    \underline{\textbf{Input}:} \\
    S1\_volume, S2\_volume, S1\_30\_ema, S2\_30\_ema, S1\_30\_rsi, S2\_30\_rsi, \\
    S1\_30\_vol, S2\_30\_vol, S1\_60\_ema, S2\_60\_ema, S1\_60\_rsi, S2\_60\_rsi, \\
    S1\_60\_vol, S2\_60\_vol, S1\_240\_ema, S2\_240\_ema, S1\_240\_rsi, S2\_240\_rsi, \\
    S1\_240\_vol, S2\_240\_vol, S1\_480\_ema, S2\_480\_ema, S1\_480\_rsi, S2\_480\_rsi, \\
    S1\_480\_vol, S2\_480\_vol, S1\_720\_ema, S2\_720\_ema, S1\_720\_rsi, S2\_720\_rsi, \\
    S1\_720\_vol, S2\_720\_vol, S1\_1440\_ema, S2\_1440\_ema, S1\_1440\_rsi, \\
    S2\_1440\_rsi, S1\_1440\_vol, S2\_1440\_vol, Spread\_open, Spread\_high, \\
    Spread\_low, Spread\_close (S1 stands for ZIL S2 stands for EOS) \\
    \\
    \underline{\textbf{Layers}:}
    \begin{enumerate}
        \item LSTM layer, 256 neurons with return sequences enabled
        \item LSTM layer, 256 neurons
        \item Dropout layer, rate = 0.2
        \item Dense layer, output shape = 1
    \end{enumerate}
    \\
    Loss function: mean square error (mse) \\
    Total trainable parameters: 831,745
    Total non-trainable parameters: 0
    
    \subsubsection{Optimizers Selection}
    As the standard deviation of the volume is larger than that of the spread of the closing price, using volume as the output variable can evaluate  performance of the LSTM model. Using different optimizers can widen the performance differences. The optimizers used in the evaluation are Adam, SGD and the RMSprop. The evaluation metrics are the time taken\footnote{The model is trained solely on a CPU, 7-th generation Intel i5} to train, the accuracy (root mean square error) of the prediction in the testing data, and the detail that can be predicted in the testing data.
    
    \begin{enumerate}
        \item
        {
            \textbf{Adam}: \\
            Adam is the algorithm that combines the concept of the momentum and the RMSprop algorithm. The advantage is that since it is the upgrade version of the momentum and RMSprop, the variation when approaching to the target will be smaller and the convergence rate will be higher. However, since the number of parameters needed to calculate is greater, the time taken to do the calculation in each step should be greater than the former. \\
            \\
            \textbf{Evaluation}: \\
            Accuracy: 0.0324 \\
            Time taken: 5 hours and 12 minutes
            \begin{figure}[H]
                \centering
                \includegraphics[width = 0.5\textwidth]{crypto/plot/LSTM_Adam.png}
                \caption{LSTM with Adam}
                \label{fig:LSTM_Adam}
            \end{figure}
            Using Adam can catch the overall movement of the true value. However, it cannot catch a sudden increase between 5,000 and 10,000 time steps. The model using Adam can catch the change between 15,000 and 20,000 time steps although the predicted magnitude and the true magnitude has an observable difference. In conclusion, using Adam gives an overall nice prediction and can predict the general shape of the true value.
        }
        
        \item
        {
            \textbf{SGD}: \\
            Stochastic Gradient Descent is the algorithm that is the upgrade of the Gradient Descent in terms of the computational power required in each update step. It applies the theory of the Law of Large Number so that although the number of data required in each step is very small, as the iteration is large, the result will converge to a nice manner. It is a computational power focus algorithm but not a performance focus algorithm. \\
            \\
            \textbf{Evaluation}: \\
            Accuracy: 0.0461 \\
            Time taken: 6 hours and 14 minutes
            \begin{figure}[H]
                \centering
                \includegraphics[width = 0.5\textwidth]{crypto/plot/LSTM_SGD.png}
                \caption{LSTM with SGD}
                \label{fig:LSTM_SGD}
            \end{figure}
            The model using SGD is performing worst the using Adam. Although it still cannot catch the sudden increase between 5,000 and 10,000 time steps, the magnitude difference between the predicted value and the true value is very large between 15,000 and 20,000 time steps. The model using SGD cannot give a prediction of the overall shape of the true value. It can just tell the direction of the movement of the true value but perform poorly in magnitude. In other words, it just gives a very conservative prediction.
        }
        
        \item
        {
            \textbf{RMSprop}: \\
            RMSprop is the upgrade version of the momentum algorithm. It tackles the large variation problem when converging to the target and increases the convergence rate using the combined concept of the weighted mean and differentiation. Compared to the Adam, it calculated less parameter in each time steps, so the computational power required for each time step is smaller than that of the Adam. \\
            \\
            \textbf{Evaluation}: \\
            Accuracy: 0.0458 \\
            Time taken: 6 hours and 1 minute
            \begin{figure}[H]
                \centering
                \includegraphics[width = 0.5\textwidth]{crypto/plot/LSTM_RMSprop.png}
                \caption{LSTM with RMSprop}
                \label{fig:LSTM_RMSprop}
            \end{figure}
            Using RMSprop gives the predicted value in an opposite side of the true value in many parts. In the time steps between 15,000 and 20,000, it gives a negative predicted value although the true value has a great positive value. Also, it gives an opposite direction prediction in the time step between 5,000 and 10,000. However, it performs better than the other model in the second large increase of the value. The predicted magnitude is very close to the true value. In general, the model does not perform well since the shape of the predicted value deviates a lot from the true value.
        }
    \end{enumerate}
    After comparing three optimizers, we are going to use Adam as the optimizer for our LSTM model.
    
    \subsubsection{Final Model Training}
    From the previous section, the optimizer used for training the model is the Adam. The time taken for training is consistent to the previous that it takes around 5 hours. The root mean square error is 0.0025.
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.5\textwidth]{crypto/plot/test_closing.png}
        \caption{Final Model Training}
        \label{fig:final_training}
    \end{figure}
    \noindent
    From the graph, the model fits very well that the model gives overall shape of the training data. Therefore, the complexity is enough for the model to fit the training dataset.
    
    \subsubsection{Final Model Testing}
        \begin{figure}[H]
        \centering
        \includegraphics[width = 0.5\textwidth]{crypto/plot/test_spread.png}
        \caption{Final Model Testing}
        \label{fig:final_testing}
    \end{figure}
    \noindent
    The root mean square error is 0.0214 \\
    From the graph, the model can predict the overall shape of the true value. The prediction deviates a little bit from the true value in the time steps around 10,000. However, the model can predict the sharp drop of the true in the time steps around 15,000. The overall performance of the model is satisfactory.
    
    % \subsubsection{Comparsion with Baseline Model}
    % depends whether have time – use the exact same set of feature to fit into a baseline model like ARIMA
    
    \subsection{Backtesting}
    
    \subsubsection{Strategy}
    To utilize our model prediction, the following strategy would be used. After we found the most cointegrated pair based on t-statistic, we calculate the coming 30 minutes percentage change ($p$) of the spread every 10 minutes and set a threshold ($e$) that is equal to 2 times the standard deviation of $p$. We buy the spread when $p > e$ and sell the spread when $p < -e$. If we are having positive position and predicting $p < -e$, we will sell all the position and sell a unit of the spread, and vice versa. The table below shows the first 3 and the last 3 trades where the return has considered the transaction cost (0.1\%). 
    \begin{table}[H]
        \centering
        \begin{tabular}{lllllll}
            TradeID & Action & Open\_time         & Open\_price & Close\_time        & Close\_price & return     \\
            1       & Buy    & 18/3/2022 11:11:00 & -78.38774   & 18/3/2022 11:41:00 & -78.425718   & 0.99851552 \\
            2       & Sell   & 20/3/2022 09:01:00 & -85.064713  & 20/3/2022 09:31:00 & -83.951233   & 0.9859102  \\
            3       & Buy    & 20/3/2022 09:31:00 & -83.951233  & 20/3/2022 10:01:00 & -83.98921    & 0.99854763 \\
            94      & Buy    & 30/3/2022 14:41    & -113.35343  & 30/3/2022 15:11    & -111.25685   & 1.01749594 \\
            95      & Sell   & 30/3/2022 15:31    & -111.40513  & 30/3/2022 16:01    & -113.0473    & 1.0137405  \\
            96      & Sell   & 31/3/2022 4:01     & -112.35409  & 31/3/2022 4:31     & -112.69271   & 1.00201393
        \end{tabular}
        \caption{Samples of Trading Detail}
        \label{tab:crypto_trading}
    \end{table}
    
    \subsubsection{Out-of-sample Testing Performance}
    \begin{figure}[H]
        \centering
        \includegraphics[width = 0.8\textwidth]{crypto/plot/backtest.png}
        \caption{Pairs Trading Backtesting}
        \label{fig:crypto_backtest}
    \end{figure}
    
    \begin{table}[H]
        \centering
        \begin{tabular}{l|cc}
                                                                                                    & Pair Trade strategy with   LSTM & Buy and Hold Bitcoin \\
            \hline \hline
            Return in 15 days                                                                       & 14.9556\%                       & 17.0747\%            \\
            \hline
            \begin{tabular}[c]{@{}l@{}}Market (BTC) Corr.    \\ (using daily return)\end{tabular} & 0.51879                         & 1                    \\
            \hline
            Daily return sd.                                                                        & 0.1666\%                        & 1.9125\%             \\
            \hline
            \begin{tabular}[c]{@{}l@{}}Annualized Sharpe    \\ (risk-free rate = 0)\end{tabular}  & 1920                            & 1009                 \\
            \hline
            Maximum drawdown                                                                        & 0.356\%                         & 11.4711\%           
        \end{tabular}
        \caption{Portfolio Statistics of the Strategies}
        \label{tab:crypto_strategy}
    \end{table}
    We set the initial capital to be equal to 1200 which enough us to buy 15 units of spread at the beginning minute. We emphasize that the equity curve above is out-of-sample and transaction cost included. \\
    \\
    From the above table, both strategies performed very well in these 15 days. Both having a double-digit return within half of a month. It is not so surprising that Buy and Hold Bitcoin outperforms our proposed strategy. One explanation could probably be that the pair trade strategy trades frequently which cost a lot transaction fee. Yet it is a surprise that the pair trade strategy has extremely small daily return standard deviation and Max. drawdown. The return sd of this strategy is 11.5 times less than the market. \\
    \\
    The market correlation of the proposed strategy is satisfactory. From figure \ref{fig:crypto_backtest}, our strategy did not lose much when the market is not performing well during March 17 to March 18 and even make some money during March 22 to March 23 when there is a 3\% drawdown in market. 
    
    \subsection{Conclusion}
    We proposed a new perspective of constructing a market neutral strategy for algo-trading. This strategy gives a decent return with a small volatility. But since we are having an extraordinary bull market in the test period. The strategy should be further tested for a longer period. Finally, we would like to mention that our strategy is not restricted to a weekly manner nor one single pair. It can be combined with a portfolio management algorithm to diversify asset specific risk. 
    
    \newpage
    % \thispagestyle{empty}
    % \pagenumbering{gobble} 
    \section{Bibliography}
    \bibliographystyle{unsrt}
    % \bibliographystyle{plain} % We choose the "plain" reference style
    \bibliography{refs} % Entries are in the refs.bib file
    
\end{document}